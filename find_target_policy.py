#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Fri Aug 12 16:59:27 2022

@author: Sourav
"""
import numpy as np
from matplotlib import pyplot as plt
class Target_Policy:
    '''
        First we create an initiualizer function namely a constructor to initialize the variables
        with initial data values
    '''
    def __init__(self,S,A,P,R,start):
        self.S=S # represant the states of the MDP
        self.nS = len(S) # Reperesants the number of states of the MDP
        self.nA = len(A);# Represants the number of actions in the MDP
        self.P=P # Represants the true Probability function
        self.R=R # Represants the true Reward  function
        self.A=A;# Represnats the Action Space
        self.K_pol = []; # To store all the policies
        self.s_start=start # Store the start state 
    '''
        In the generate_next_state(), we are generating our next state to be visited based on the following input parameters
        s : Current state
        a : Current action
    '''    
    def generate_next_state(self,s,a):
        #p = np.zeros(self.nS)
        p = self.P[a][s][:] # extrcat all the probabilities of the different statestransition given current s and a
        #print(p);
        return (np.argmax(np.random.multinomial(1,p)))
    
    '''
        Single function to find the plot between the cumulative regret generated by different algorithms
        Parameters:
            reg_list : A list containing the regret value at different runs instances averaged over several time
    '''    
    def plot_data(self,reg_list):
        plt.plot(np.arange(len(reg_list)),np.cumsum(np.array(reg_list)),marker = '+'); 
    '''
        Function to find the optimum policy out of the K policies found out.
        Parameters:
            runs : To find for how many runs the current policy to be runned
            T : Each run consisiting of how many time steps to find the average reward for each policy in one run
            Time complexity : O(#(policies) x #(episode runs) x #(number of Time steps in one episode))
    '''
    def find_optimum_policy(self,runs,T):
        self.find_policies(); #Call the find_policies() to find all the policies and store it in 'self.K' list
        final_R = np.zeros(len(self.K_pol)); # Array to store the final averaged reward of each policy
        for l_pol in range(len(self.K_pol)):
            pol = self.K_pol[l_pol] # Select one policy to run in one episode
            #state = self.s_start;
            #print("policy selected:",pol) 
            for run in range(runs):
                state =self.s_start;
                run_reward = np.zeros(T); # array to store the reward obtained at each time step
                for t in range(T):
                    a = int(pol[state]); # Action accoriding to the slected policy
                    next_state = self.generate_next_state(state,a); # generate the next state
                    #print("running time,next_state:",t,next_state);
                    run_reward[t] = self.R[a,state,next_state]; #Find the reward obtained at each time step
                    state = next_state; # store the visited state as the current state
                #print("===========================================================");
                #print("One_run completed");
                #print("===========================================================");
                avg_rew = np.mean(run_reward); #Find the average reward after T time running 
                final_R[l_pol]+=avg_rew; #Store the average reward in final_R list adding the conetents present in it in previous instances
            final_R[l_pol] = final_R[l_pol]/runs; # Averaging the rewards obtained over #(episode runs)
        for l_pol in range(len(self.K_pol)):
            print(self.K_pol[l_pol],"    ====>    ",final_R[l_pol]); # Display the the expected reward for each policy
        return (np.min(final_R),np.argmin(final_R),self.K_pol[np.argmin(final_R)]);# Return the minimum reward, the policy number which gives the minimum reward and the policy that gives minimum reward
    '''
        The find_policies() exploits the MDP structure as a threshold structure and obtains
    '''                
    def find_policies(self):
        self.K_pol = [];
        pol=np.zeros(self.nS) # First policy is all 0's
        self.K_pol.append(np.array(pol)); # append it to our K_policy list namely self.K
        for i in reversed(range(self.nS)):
            pol[i] = 1; # Come from the end and since the structure is thresholding nature so make each position 1 from 0 and append the same
            print(pol);
            self.K_pol.append(np.array(pol));
        print(len(self.K_pol)," policies found");
    
    '''
        This function run_algo() is responsible running the list of algorithms meant to be compared.
        Parameters:
            runs : Specify number of episodes runs
            Time_runs : Specify for how much time each episode should run to find maximum policy
            tau : Pass as input to pUCB and pThompson algorithms to specify practical infinite runs
            beta : Pass as input to pUCB algorithm to find the confidence bound c[k] = beta[t] * sqrt(2 * log(t)/n(k))
            regret_runs : How many times the algorithm to be learned so that it can send the cumulative reward in order to find 
            the regret.
    
    def run_algo(self,runs,Time_runs,tau,beta,regret_runs):
        max_expected_rew,max_expected_policy_number,max_policy = self.find_maximum_policy(runs,Time_runs)
        reg_list = [];
        for run in range(regret_runs):
            ob1 = pUCB(Time_runs,self.K_pol,self.P,self.R,self.nS,self.nA,beta,self.s_start,tau);
            cum_ret,n_episodes = ob1.run_algo()
            reg_list.append(max_expected_rew * n_episodes - cum_ret);
        self.plot_data(reg_list);
        print(len(reg_list))'''
